# ğŸ© Escrevendo como Lewis Carroll: GeraÃ§Ã£o de Texto com LSTM

Este projeto utiliza **Redes Neurais Recorrentes (RNN)**, especificamente a arquitetura **Long Short-Term Memory (LSTM)**, para aprender o estilo literÃ¡rio de **Lewis Carroll** e gerar novos textos inspirados na obra clÃ¡ssica **_Alice no PaÃ­s das Maravilhas_**.

---

## ğŸ“– VisÃ£o Geral

O objetivo Ã© criar um modelo de **prediÃ§Ã£o caractere por caractere**.  
Dada uma sequÃªncia de texto, o modelo aprende a prever o prÃ³ximo caractere mais provÃ¡vel, possibilitando a geraÃ§Ã£o de parÃ¡grafos inteiros que mimetizam o vocabulÃ¡rio, a estrutura sintÃ¡tica e o ritmo narrativo do autor original.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3**
- **TensorFlow / Keras** â€” construÃ§Ã£o e treinamento da rede neural
- **NumPy & Pandas** â€” manipulaÃ§Ã£o e preparaÃ§Ã£o dos dados
- **Google Colab** â€” ambiente de desenvolvimento e execuÃ§Ã£o

---

## ğŸ—ï¸ Arquitetura do Modelo

O modelo foi implementado utilizando a **API Funcional do Keras** e Ã© composto por trÃªs camadas principais:

1. **Embedding**  
   Camada de entrada responsÃ¡vel por mapear os IDs dos caracteres em vetores densos.

2. **LSTM**  
   Camada recorrente com **1024 unidades**, encarregada de aprender dependÃªncias de longo prazo na sequÃªncia textual.

3. **Dense**  
   Camada de saÃ­da que projeta a representaÃ§Ã£o aprendida para o tamanho do vocabulÃ¡rio (logits).

---

## ğŸš€ Fluxo de Trabalho

### 1. PrÃ©-processamento

- **VetorizaÃ§Ã£o**  
  ConversÃ£o do texto original em IDs numÃ©ricos utilizando `tf.keras.layers.StringLookup`.

- **CriaÃ§Ã£o de SequÃªncias**  
  O texto foi segmentado em janelas deslizantes de **100 caracteres** para treinamento supervisionado.

- **Batching**  
  OrganizaÃ§Ã£o dos dados em batches de **64 sequÃªncias**, com *shuffle* e *prefetch* para otimizaÃ§Ã£o de desempenho.

---

### 2. Treinamento

- **FunÃ§Ã£o de Perda (Loss Function)**  
  `SparseCategoricalCrossentropy` (a partir de logits).

- **Otimizador**  
  `Adam`.

- **Ã‰pocas**  
  Treinamento realizado por **20 Ã©pocas**.

- **Checkpoints**  
  Salvamento automÃ¡tico dos pesos do modelo para retomada do treinamento ou inferÃªncia posterior.

---

### 3. GeraÃ§Ã£o de Texto

A geraÃ§Ã£o de texto Ã© realizada por meio de **amostragem com Temperatura**:

- **Temperatura alta** â†’ textos mais criativos e imprevisÃ­veis  
- **Temperatura baixa** â†’ textos mais conservadores e repetitivos

---

## ğŸ“Š Resultados

ApÃ³s o treinamento, o modelo foi capaz de gerar fragmentos textuais contendo:

- DiÃ¡logos estruturados  
- Nomes de personagens como *Alice*, *The King* e *The Queen*  
- Uso consistente de pontuaÃ§Ã£o e quebras de linha caracterÃ­sticas da obra original  

**Exemplo de saÃ­da:**

```text
Alice: 'Oh! What I'd non. wet tast is be ding one boling
and retting or any--and itself so you filling thome...
